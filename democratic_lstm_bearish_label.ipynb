{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a442627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dtale as dt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b3e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa83e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6211d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17dc07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.read_pickle('y_train.pkl')\n",
    "y_test=pd.read_pickle('y_test.pkl')\n",
    "x_test=pd.read_pickle('x_test.pkl')\n",
    "x_train=pd.read_pickle('x_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5b9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_col='log_bearish_week'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebce1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y=y_test[pred_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d49b1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1=x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59562ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=y_train[pred_col]\n",
    "#Y_train=df['Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd00fd",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00626e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        #normalize\n",
    "\n",
    "    \n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        new_shape = (len(Y), 1)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "        self.Y = self.Y.view(new_shape)\n",
    "        \n",
    "    \n",
    "        \n",
    "        #self.Y = self.Y.view(new_shape)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86026c55",
   "metadata": {},
   "source": [
    "# Normalize the X data\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2fd11d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sc.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = StandardScaler()\n",
    "scaled_fit=scaler.fit_transform(training_data1)\n",
    "        \n",
    "\n",
    "joblib.dump(scaler, 'sc.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "958bf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler_y = StandardScaler()\n",
    "scaled_fit_y=Y_train.values.reshape(-1, 1)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e6bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is a DataFrame with 5 columns and Y is a DataFrame with 1 column\n",
    "#train_data = MyDataset(scaled_fit, Y_train.values.reshape(-1, 1))\n",
    "train_data = MyDataset(scaled_fit, scaled_fit_y)\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=32) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86ff356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_normalized_change</th>\n",
       "      <th>log_price_range</th>\n",
       "      <th>log_price_range_high</th>\n",
       "      <th>log_price_range_low</th>\n",
       "      <th>log_premarket_changes</th>\n",
       "      <th>log_Smart_Money</th>\n",
       "      <th>log_volume_deviation</th>\n",
       "      <th>log_norm_avg_deviation_200</th>\n",
       "      <th>log_norm_avg_deviation_300</th>\n",
       "      <th>log_ha_change</th>\n",
       "      <th>log_ha_change_intra</th>\n",
       "      <th>log_ha_change_intra_high</th>\n",
       "      <th>log_ha_high_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.018210</td>\n",
       "      <td>0.022816</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>0.016633</td>\n",
       "      <td>-0.001845</td>\n",
       "      <td>-0.016364</td>\n",
       "      <td>0.594377</td>\n",
       "      <td>0.058461</td>\n",
       "      <td>0.094445</td>\n",
       "      <td>-0.011852</td>\n",
       "      <td>-0.015852</td>\n",
       "      <td>0.025832</td>\n",
       "      <td>0.025832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.012927</td>\n",
       "      <td>0.013987</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.009713</td>\n",
       "      <td>0.273865</td>\n",
       "      <td>0.070581</td>\n",
       "      <td>0.106799</td>\n",
       "      <td>-0.002194</td>\n",
       "      <td>-0.010152</td>\n",
       "      <td>0.017736</td>\n",
       "      <td>0.017736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>-0.231275</td>\n",
       "      <td>0.071486</td>\n",
       "      <td>0.107933</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.008260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-0.010966</td>\n",
       "      <td>0.013219</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.007461</td>\n",
       "      <td>-0.004173</td>\n",
       "      <td>-0.006793</td>\n",
       "      <td>0.517538</td>\n",
       "      <td>0.059751</td>\n",
       "      <td>0.096423</td>\n",
       "      <td>-0.005753</td>\n",
       "      <td>-0.004997</td>\n",
       "      <td>0.010349</td>\n",
       "      <td>0.013219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.014857</td>\n",
       "      <td>0.015574</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.296428</td>\n",
       "      <td>0.073756</td>\n",
       "      <td>0.110679</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.006718</td>\n",
       "      <td>0.015574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.008479</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>-0.001433</td>\n",
       "      <td>-0.172502</td>\n",
       "      <td>0.033980</td>\n",
       "      <td>-0.002405</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.015805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263</th>\n",
       "      <td>-0.015447</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.015324</td>\n",
       "      <td>0.208525</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>-0.017319</td>\n",
       "      <td>-0.009913</td>\n",
       "      <td>-0.002042</td>\n",
       "      <td>0.011098</td>\n",
       "      <td>0.017550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>-0.173034</td>\n",
       "      <td>0.019831</td>\n",
       "      <td>-0.015107</td>\n",
       "      <td>-0.007116</td>\n",
       "      <td>-0.008137</td>\n",
       "      <td>0.012697</td>\n",
       "      <td>0.012697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>-0.018622</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.020676</td>\n",
       "      <td>0.202728</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>-0.033089</td>\n",
       "      <td>-0.006484</td>\n",
       "      <td>-0.010561</td>\n",
       "      <td>0.024035</td>\n",
       "      <td>0.027653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>-0.014535</td>\n",
       "      <td>0.022741</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.017206</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.013078</td>\n",
       "      <td>0.651889</td>\n",
       "      <td>-0.013339</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.018441</td>\n",
       "      <td>-0.023735</td>\n",
       "      <td>0.034797</td>\n",
       "      <td>0.034797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3968 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      log_normalized_change  log_price_range  log_price_range_high  \\\n",
       "299               -0.018210         0.022816              0.006183   \n",
       "300                0.012927         0.013987              0.010706   \n",
       "301                0.001720         0.008260              0.006735   \n",
       "302               -0.010966         0.013219              0.005758   \n",
       "303                0.014857         0.015574              0.010378   \n",
       "...                     ...              ...                   ...   \n",
       "4262               0.000693         0.008479              0.005908   \n",
       "4263              -0.015447         0.017550              0.000618   \n",
       "4264               0.001631         0.007836              0.003308   \n",
       "4265              -0.018622         0.027653              0.004343   \n",
       "4266              -0.014535         0.022741              0.005535   \n",
       "\n",
       "      log_price_range_low  log_premarket_changes  log_Smart_Money  \\\n",
       "299              0.016633              -0.001845        -0.016364   \n",
       "300              0.003281               0.003214         0.009713   \n",
       "301              0.001525              -0.000728         0.002449   \n",
       "302              0.007461              -0.004173        -0.006793   \n",
       "303              0.005196               0.005664         0.009193   \n",
       "...                   ...                    ...              ...   \n",
       "4262             0.002571               0.002125        -0.001433   \n",
       "4263             0.016932              -0.000124        -0.015324   \n",
       "4264             0.004528               0.000301         0.001329   \n",
       "4265             0.023310               0.002053        -0.020676   \n",
       "4266             0.017206              -0.001457        -0.013078   \n",
       "\n",
       "      log_volume_deviation  log_norm_avg_deviation_200  \\\n",
       "299               0.594377                    0.058461   \n",
       "300               0.273865                    0.070581   \n",
       "301              -0.231275                    0.071486   \n",
       "302               0.517538                    0.059751   \n",
       "303               0.296428                    0.073756   \n",
       "...                    ...                         ...   \n",
       "4262             -0.172502                    0.033980   \n",
       "4263              0.208525                    0.018386   \n",
       "4264             -0.173034                    0.019831   \n",
       "4265              0.202728                    0.001120   \n",
       "4266              0.651889                   -0.013339   \n",
       "\n",
       "      log_norm_avg_deviation_300  log_ha_change  log_ha_change_intra  \\\n",
       "299                     0.094445      -0.011852            -0.015852   \n",
       "300                     0.106799      -0.002194            -0.010152   \n",
       "301                     0.107933       0.006602             0.001513   \n",
       "302                     0.096423      -0.005753            -0.004997   \n",
       "303                     0.110679       0.004595             0.002093   \n",
       "...                          ...            ...                  ...   \n",
       "4262                   -0.002405       0.008425             0.015805   \n",
       "4263                   -0.017319      -0.009913            -0.002042   \n",
       "4264                   -0.015107      -0.007116            -0.008137   \n",
       "4265                   -0.033089      -0.006484            -0.010561   \n",
       "4266                   -0.046932      -0.018441            -0.023735   \n",
       "\n",
       "      log_ha_change_intra_high  log_ha_high_low  \n",
       "299                   0.025832         0.025832  \n",
       "300                   0.017736         0.017736  \n",
       "301                   0.001932         0.008260  \n",
       "302                   0.010349         0.013219  \n",
       "303                   0.006718         0.015574  \n",
       "...                        ...              ...  \n",
       "4262                  0.000000         0.021232  \n",
       "4263                  0.011098         0.017550  \n",
       "4264                  0.012697         0.012697  \n",
       "4265                  0.024035         0.027653  \n",
       "4266                  0.034797         0.034797  \n",
       "\n",
       "[3968 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84769f2f",
   "metadata": {},
   "source": [
    "# Pytorch setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f5fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to match the expected shape of (batch_size, sequence_length, input_size)\n",
    "        x = x.view(-1, 1, x.shape[-1])\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        # Apply activation function\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output[:, -1, :])\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        output = self.sigmoid(output)\n",
    " \n",
    "        return torch.squeeze(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc58cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 35000\n",
    "learning_rate = 0.0005\n",
    "num_layers=5\n",
    "input_size = len(x_train.columns)\n",
    "hidden_size = 90\n",
    "output_size = 1\n",
    "dropout=.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dd83d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_list_to_dataframe(arrays,length):\n",
    "    # Convert the list of arrays to a single 2D array\n",
    "    array_2d = np.concatenate(arrays, axis=1)\n",
    "    \n",
    "    # Reshape the 2D array into a 5-column array\n",
    "    array_5col = np.reshape(array_2d, (-1, length))\n",
    "    \n",
    "    # Create a DataFrame from the 5-column array\n",
    "    df = pd.DataFrame(array_5col)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2040bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "certainty_2sd_counts=pd.DataFrame()\n",
    "certainty_4sd_counts=pd.DataFrame()\n",
    "certainty_5sd_counts=pd.DataFrame()\n",
    "certainty_6sd_counts=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5510f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db282c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffwa/anaconda3/envs/DL_new/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.67769 0\n",
      "Epoch: 1000, loss: 0.21200 0\n",
      "Epoch: 2000, loss: 0.11343 0\n",
      "Epoch: 3000, loss: 0.03641 0\n",
      "Epoch: 4000, loss: 0.01845 0\n",
      "Epoch: 5000, loss: 0.00243 0\n",
      "Epoch: 6000, loss: 0.00035 0\n",
      "Epoch: 7000, loss: 0.00044 0\n",
      "Epoch: 8000, loss: 0.03899 0\n",
      "Epoch: 9000, loss: 0.00169 0\n",
      "Epoch: 10000, loss: 0.00188 0\n",
      "Epoch: 11000, loss: 0.00121 0\n",
      "Epoch: 12000, loss: 0.00027 0\n",
      "Epoch: 13000, loss: 0.00026 0\n",
      "Epoch: 14000, loss: 0.00011 0\n",
      "Epoch: 15000, loss: 0.00013 0\n",
      "Epoch: 16000, loss: 0.00014 0\n",
      "Epoch: 17000, loss: 0.00002 0\n",
      "Epoch: 18000, loss: 0.00048 0\n",
      "Epoch: 19000, loss: 0.00003 0\n",
      "Epoch: 20000, loss: 0.00026 0\n",
      "Epoch: 21000, loss: 0.00003 0\n",
      "Epoch: 22000, loss: 0.00007 0\n",
      "Epoch: 23000, loss: 0.00004 0\n",
      "Epoch: 24000, loss: 0.00003 0\n",
      "Epoch: 25000, loss: 0.00028 0\n",
      "Epoch: 26000, loss: 0.00001 0\n",
      "Epoch: 27000, loss: 0.00026 0\n",
      "Epoch: 28000, loss: 0.00002 0\n",
      "Epoch: 29000, loss: 0.00001 0\n",
      "Epoch: 30000, loss: 0.00002 0\n",
      "Epoch: 31000, loss: 0.00004 0\n",
      "Epoch: 32000, loss: 0.00002 0\n",
      "Epoch: 33000, loss: 0.00001 0\n",
      "Epoch: 34000, loss: 0.00001 0\n",
      "Epoch: 0, loss: 0.70792 1\n",
      "Epoch: 1000, loss: 0.20396 1\n",
      "Epoch: 2000, loss: 0.13037 1\n",
      "Epoch: 3000, loss: 0.04066 1\n",
      "Epoch: 4000, loss: 0.08024 1\n",
      "Epoch: 5000, loss: 0.00275 1\n",
      "Epoch: 6000, loss: 0.00050 1\n",
      "Epoch: 7000, loss: 0.26143 1\n",
      "Epoch: 8000, loss: 0.00180 1\n",
      "Epoch: 9000, loss: 0.00106 1\n",
      "Epoch: 10000, loss: 0.00030 1\n",
      "Epoch: 11000, loss: 0.00013 1\n",
      "Epoch: 12000, loss: 0.00858 1\n",
      "Epoch: 13000, loss: 0.00065 1\n",
      "Epoch: 14000, loss: 0.00030 1\n",
      "Epoch: 15000, loss: 0.00007 1\n",
      "Epoch: 16000, loss: 0.00013 1\n",
      "Epoch: 17000, loss: 0.00004 1\n",
      "Epoch: 18000, loss: 0.00002 1\n",
      "Epoch: 19000, loss: 0.00065 1\n",
      "Epoch: 20000, loss: 0.00010 1\n",
      "Epoch: 21000, loss: 0.00004 1\n",
      "Epoch: 22000, loss: 0.00017 1\n",
      "Epoch: 23000, loss: 0.00004 1\n",
      "Epoch: 24000, loss: 0.00066 1\n",
      "Epoch: 25000, loss: 0.00005 1\n",
      "Epoch: 26000, loss: 0.00001 1\n",
      "Epoch: 27000, loss: 0.00001 1\n",
      "Epoch: 28000, loss: 0.00037 1\n",
      "Epoch: 29000, loss: 0.00006 1\n",
      "Epoch: 30000, loss: 0.00011 1\n",
      "Epoch: 31000, loss: 0.00003 1\n",
      "Epoch: 32000, loss: 0.00002 1\n",
      "Epoch: 33000, loss: 0.00002 1\n",
      "Epoch: 34000, loss: 0.00024 1\n",
      "Epoch: 0, loss: 0.71761 2\n",
      "Epoch: 1000, loss: 0.20753 2\n",
      "Epoch: 2000, loss: 0.14535 2\n",
      "Epoch: 3000, loss: 0.11986 2\n",
      "Epoch: 4000, loss: 0.06310 2\n",
      "Epoch: 5000, loss: 0.02471 2\n",
      "Epoch: 6000, loss: 0.00819 2\n",
      "Epoch: 7000, loss: 0.00464 2\n",
      "Epoch: 8000, loss: 0.01466 2\n",
      "Epoch: 9000, loss: 0.00559 2\n",
      "Epoch: 10000, loss: 0.00374 2\n",
      "Epoch: 11000, loss: 0.02084 2\n",
      "Epoch: 12000, loss: 0.00243 2\n",
      "Epoch: 13000, loss: 0.00182 2\n",
      "Epoch: 14000, loss: 0.00179 2\n",
      "Epoch: 15000, loss: 0.00115 2\n",
      "Epoch: 16000, loss: 0.00053 2\n",
      "Epoch: 17000, loss: 0.00049 2\n",
      "Epoch: 18000, loss: 0.00099 2\n",
      "Epoch: 19000, loss: 0.00024 2\n",
      "Epoch: 20000, loss: 0.00192 2\n",
      "Epoch: 21000, loss: 0.00020 2\n",
      "Epoch: 22000, loss: 0.00015 2\n",
      "Epoch: 23000, loss: 0.00017 2\n",
      "Epoch: 24000, loss: 0.00004 2\n",
      "Epoch: 25000, loss: 0.00003 2\n",
      "Epoch: 26000, loss: 0.00004 2\n",
      "Epoch: 27000, loss: 0.00010 2\n",
      "Epoch: 28000, loss: 0.18259 2\n",
      "Epoch: 29000, loss: 0.00038 2\n",
      "Epoch: 30000, loss: 0.00007 2\n",
      "Epoch: 31000, loss: 0.00002 2\n",
      "Epoch: 32000, loss: 0.00003 2\n",
      "Epoch: 33000, loss: 0.00104 2\n",
      "Epoch: 34000, loss: 0.00003 2\n",
      "Epoch: 0, loss: 0.70865 3\n",
      "Epoch: 1000, loss: 0.20680 3\n",
      "Epoch: 2000, loss: 0.10987 3\n",
      "Epoch: 3000, loss: 0.07079 3\n",
      "Epoch: 4000, loss: 0.07162 3\n",
      "Epoch: 5000, loss: 0.03093 3\n",
      "Epoch: 6000, loss: 0.18653 3\n",
      "Epoch: 7000, loss: 0.03078 3\n",
      "Epoch: 8000, loss: 0.03130 3\n",
      "Epoch: 9000, loss: 0.05815 3\n",
      "Epoch: 10000, loss: 0.00811 3\n",
      "Epoch: 11000, loss: 0.00249 3\n",
      "Epoch: 12000, loss: 0.00197 3\n",
      "Epoch: 13000, loss: 0.00044 3\n",
      "Epoch: 14000, loss: 0.01843 3\n",
      "Epoch: 15000, loss: 0.05227 3\n",
      "Epoch: 16000, loss: 0.00118 3\n",
      "Epoch: 17000, loss: 0.00084 3\n",
      "Epoch: 18000, loss: 0.00030 3\n",
      "Epoch: 19000, loss: 0.00211 3\n",
      "Epoch: 20000, loss: 0.00015 3\n",
      "Epoch: 21000, loss: 0.00008 3\n",
      "Epoch: 22000, loss: 0.22052 3\n",
      "Epoch: 23000, loss: 0.00034 3\n",
      "Epoch: 24000, loss: 0.00062 3\n",
      "Epoch: 25000, loss: 0.00006 3\n",
      "Epoch: 26000, loss: 0.00045 3\n",
      "Epoch: 27000, loss: 0.00005 3\n",
      "Epoch: 28000, loss: 0.00004 3\n",
      "Epoch: 29000, loss: 0.00008 3\n",
      "Epoch: 30000, loss: 0.00002 3\n",
      "Epoch: 31000, loss: 0.00002 3\n",
      "Epoch: 32000, loss: 0.01154 3\n",
      "Epoch: 33000, loss: 0.00004 3\n",
      "Epoch: 34000, loss: 0.00025 3\n",
      "Epoch: 0, loss: 0.73618 4\n",
      "Epoch: 1000, loss: 0.19880 4\n",
      "Epoch: 2000, loss: 0.11896 4\n",
      "Epoch: 3000, loss: 0.05774 4\n",
      "Epoch: 4000, loss: 0.02441 4\n",
      "Epoch: 5000, loss: 0.16467 4\n",
      "Epoch: 6000, loss: 0.01444 4\n",
      "Epoch: 7000, loss: 0.01607 4\n",
      "Epoch: 8000, loss: 0.00634 4\n",
      "Epoch: 9000, loss: 0.06572 4\n",
      "Epoch: 10000, loss: 0.00270 4\n",
      "Epoch: 11000, loss: 0.01232 4\n",
      "Epoch: 12000, loss: 0.00223 4\n",
      "Epoch: 13000, loss: 0.00100 4\n",
      "Epoch: 14000, loss: 0.00078 4\n",
      "Epoch: 15000, loss: 0.00149 4\n",
      "Epoch: 16000, loss: 0.00086 4\n",
      "Epoch: 17000, loss: 0.01407 4\n",
      "Epoch: 18000, loss: 0.00026 4\n",
      "Epoch: 19000, loss: 0.00041 4\n",
      "Epoch: 20000, loss: 0.00031 4\n",
      "Epoch: 21000, loss: 0.00010 4\n",
      "Epoch: 22000, loss: 0.00020 4\n",
      "Epoch: 23000, loss: 0.00014 4\n",
      "Epoch: 24000, loss: 0.00018 4\n",
      "Epoch: 25000, loss: 0.00003 4\n",
      "Epoch: 26000, loss: 0.00009 4\n",
      "Epoch: 27000, loss: 0.00002 4\n",
      "Epoch: 28000, loss: 0.00011 4\n",
      "Epoch: 29000, loss: 0.00008 4\n",
      "Epoch: 30000, loss: 0.00007 4\n",
      "Epoch: 31000, loss: 0.00002 4\n",
      "Epoch: 32000, loss: 0.00005 4\n",
      "Epoch: 33000, loss: 0.00001 4\n",
      "Epoch: 34000, loss: 0.00001 4\n",
      "Epoch: 0, loss: 0.72379 5\n",
      "Epoch: 1000, loss: 0.21520 5\n",
      "Epoch: 2000, loss: 0.09681 5\n",
      "Epoch: 3000, loss: 0.05334 5\n",
      "Epoch: 4000, loss: 0.02792 5\n",
      "Epoch: 5000, loss: 0.01599 5\n",
      "Epoch: 6000, loss: 0.01622 5\n",
      "Epoch: 7000, loss: 0.00188 5\n",
      "Epoch: 8000, loss: 0.00065 5\n",
      "Epoch: 9000, loss: 0.00032 5\n",
      "Epoch: 10000, loss: 0.00035 5\n",
      "Epoch: 11000, loss: 0.00016 5\n",
      "Epoch: 12000, loss: 0.00081 5\n",
      "Epoch: 13000, loss: 0.00127 5\n",
      "Epoch: 14000, loss: 0.00022 5\n",
      "Epoch: 15000, loss: 0.00026 5\n",
      "Epoch: 16000, loss: 0.00005 5\n",
      "Epoch: 17000, loss: 0.00007 5\n",
      "Epoch: 18000, loss: 0.00013 5\n",
      "Epoch: 19000, loss: 0.00007 5\n",
      "Epoch: 20000, loss: 0.00004 5\n",
      "Epoch: 21000, loss: 0.00006 5\n",
      "Epoch: 22000, loss: 0.00009 5\n",
      "Epoch: 23000, loss: 0.00009 5\n",
      "Epoch: 24000, loss: 0.00003 5\n",
      "Epoch: 25000, loss: 0.00002 5\n",
      "Epoch: 26000, loss: 0.00001 5\n",
      "Epoch: 27000, loss: 0.00005 5\n",
      "Epoch: 28000, loss: 0.00002 5\n",
      "Epoch: 29000, loss: 0.00003 5\n",
      "Epoch: 30000, loss: 0.00018 5\n",
      "Epoch: 31000, loss: 0.00001 5\n",
      "Epoch: 32000, loss: 0.00001 5\n",
      "Epoch: 33000, loss: 0.00001 5\n",
      "Epoch: 34000, loss: 0.00002 5\n",
      "Epoch: 0, loss: 0.68456 6\n",
      "Epoch: 1000, loss: 0.19125 6\n",
      "Epoch: 2000, loss: 0.11307 6\n",
      "Epoch: 3000, loss: 0.05738 6\n",
      "Epoch: 4000, loss: 0.04428 6\n",
      "Epoch: 5000, loss: 0.02751 6\n",
      "Epoch: 6000, loss: 0.01599 6\n",
      "Epoch: 7000, loss: 0.00644 6\n",
      "Epoch: 8000, loss: 0.00934 6\n",
      "Epoch: 9000, loss: 0.01319 6\n",
      "Epoch: 10000, loss: 0.00201 6\n",
      "Epoch: 11000, loss: 0.00105 6\n",
      "Epoch: 12000, loss: 0.00249 6\n",
      "Epoch: 13000, loss: 0.00022 6\n",
      "Epoch: 14000, loss: 0.00006 6\n",
      "Epoch: 15000, loss: 0.00106 6\n",
      "Epoch: 16000, loss: 0.10147 6\n",
      "Epoch: 17000, loss: 0.00044 6\n",
      "Epoch: 18000, loss: 0.00023 6\n",
      "Epoch: 19000, loss: 0.00042 6\n",
      "Epoch: 20000, loss: 0.00012 6\n",
      "Epoch: 21000, loss: 0.00005 6\n",
      "Epoch: 22000, loss: 0.00006 6\n",
      "Epoch: 23000, loss: 0.00006 6\n",
      "Epoch: 24000, loss: 0.00004 6\n",
      "Epoch: 25000, loss: 0.00038 6\n",
      "Epoch: 26000, loss: 0.00003 6\n",
      "Epoch: 27000, loss: 0.00002 6\n",
      "Epoch: 28000, loss: 0.00004 6\n",
      "Epoch: 29000, loss: 0.00027 6\n",
      "Epoch: 30000, loss: 0.00007 6\n",
      "Epoch: 31000, loss: 0.00014 6\n",
      "Epoch: 32000, loss: 0.00002 6\n",
      "Epoch: 33000, loss: 0.00026 6\n",
      "Epoch: 34000, loss: 0.00002 6\n"
     ]
    }
   ],
   "source": [
    "for j in range(7):\n",
    "    # Instantiate the model\n",
    "    model = LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers,output_size=1,dropout=dropout)#output_size=1\n",
    "    model=torch.compile(model)\n",
    "    model.to(device)\n",
    "    train_data_x=train_data.X.to(device)\n",
    "    train_data_y=train_data.Y.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.BCELoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.003,weight_decay=6e-7) \n",
    "        # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        outputs = model(train_data_x)\n",
    "        loss = criterion(outputs, train_data_y.squeeze())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Epoch: %d, loss: %1.5f\"  % (epoch, loss.item()),j)\n",
    "        \"\"\"for batch_X, batch_Y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_Y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\"\"\"\n",
    "        \n",
    "        \n",
    "    #Transform test data\n",
    "    transformedData = scaler.transform(x_test)\n",
    "    transformedData_y=test_y.values.reshape(-1, 1)\n",
    "\n",
    "    # Use the trained model for predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_data = MyDataset(transformedData, transformedData_y)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "    test_data_x=test_data.X.to(device)\n",
    "    \n",
    "    outputlist=[]\n",
    "    \n",
    "    #testing data\n",
    "    \n",
    "    model.train()\n",
    "    len_outputs=100\n",
    "    with torch.no_grad():\n",
    "        for i in range(len_outputs):\n",
    "            output = model(test_data_x)\n",
    "            outputlist.append(output)\n",
    "    model.eval()\n",
    "    \n",
    "    scaled_output_list=[]\n",
    "    for outputs in outputlist:\n",
    "        scaled_output=outputs.cpu().reshape(-1, 1)\n",
    "        scaled_output_list.append(scaled_output)\n",
    "        \n",
    "    model_test_outputs=array_list_to_dataframe(scaled_output_list,len_outputs )\n",
    "    \n",
    "    count1 = (model_test_outputs > .999).sum(axis=1)\n",
    "    count2=(model_test_outputs > .9999).sum(axis=1)\n",
    "    count3=(model_test_outputs > .99999).sum(axis=1)\n",
    "    count4=(model_test_outputs > .999999).sum(axis=1)\n",
    "    \n",
    "    \n",
    "    column_name = 'Column ' + str(j)\n",
    "    \n",
    "    certainty_2sd_counts[column_name] =count1 \n",
    "    certainty_4sd_counts[column_name] =count2\n",
    "    certainty_5sd_counts[column_name] =count3\n",
    "    certainty_6sd_counts[column_name] =count4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6782bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.000000\n",
       "1      2.857143\n",
       "2     48.571429\n",
       "3     26.428571\n",
       "4      0.000000\n",
       "5      0.428571\n",
       "6      0.000000\n",
       "7      0.000000\n",
       "8      0.000000\n",
       "9     15.428571\n",
       "10     0.000000\n",
       "11    40.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certainty_6sd_counts.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b58fdfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column 0</th>\n",
       "      <th>Column 1</th>\n",
       "      <th>Column 2</th>\n",
       "      <th>Column 3</th>\n",
       "      <th>Column 4</th>\n",
       "      <th>Column 5</th>\n",
       "      <th>Column 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>26</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>89</td>\n",
       "      <td>16</td>\n",
       "      <td>98</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column 0  Column 1  Column 2  Column 3  Column 4  Column 5  Column 6\n",
       "0          0         0         0         0         0         0         0\n",
       "1          0         0        20         0         0         0         0\n",
       "2         77         0        90        30         0        50        93\n",
       "3          0         0        98        26        61         0         0\n",
       "4          0         0         0         0         0         0         0\n",
       "5          0         0         0         3         0         0         0\n",
       "6          0         0         0         0         0         0         0\n",
       "7          0         0         0         0         0         0         0\n",
       "8          0         0         0         0         0         0         0\n",
       "9          0         0         5        76         0        27         0\n",
       "10         0         0         0         0         0         0         0\n",
       "11         0         2        89        16        98        75         0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certainty_6sd_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d40e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
