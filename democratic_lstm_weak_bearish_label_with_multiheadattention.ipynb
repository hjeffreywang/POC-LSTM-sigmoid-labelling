{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a442627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dtale as dt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b3e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa83e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6211d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17dc07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.read_pickle('y_train.pkl')\n",
    "y_test=pd.read_pickle('y_test.pkl')\n",
    "x_test=pd.read_pickle('x_test.pkl')\n",
    "x_train=pd.read_pickle('x_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a5b9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_col='log_bearish_week'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebce1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y=y_test[pred_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d49b1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1=x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59562ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=y_train[pred_col]\n",
    "#Y_train=df['Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd00fd",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00626e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        #normalize\n",
    "\n",
    "    \n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        new_shape = (len(Y), 1)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "        self.Y = self.Y.view(new_shape)\n",
    "        \n",
    "    \n",
    "        \n",
    "        #self.Y = self.Y.view(new_shape)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86026c55",
   "metadata": {},
   "source": [
    "# Normalize the X data\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2fd11d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sc.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = StandardScaler()\n",
    "scaled_fit=scaler.fit_transform(training_data1)\n",
    "        \n",
    "\n",
    "joblib.dump(scaler, 'sc.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "958bf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler_y = StandardScaler()\n",
    "scaled_fit_y=Y_train.values.reshape(-1, 1)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e6bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is a DataFrame with 5 columns and Y is a DataFrame with 1 column\n",
    "#train_data = MyDataset(scaled_fit, Y_train.values.reshape(-1, 1))\n",
    "train_data = MyDataset(scaled_fit, scaled_fit_y)\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=32) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86ff356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_normalized_change</th>\n",
       "      <th>log_price_range</th>\n",
       "      <th>log_price_range_high</th>\n",
       "      <th>log_price_range_low</th>\n",
       "      <th>log_premarket_changes</th>\n",
       "      <th>log_Smart_Money</th>\n",
       "      <th>log_volume_deviation</th>\n",
       "      <th>log_norm_avg_deviation_200</th>\n",
       "      <th>log_norm_avg_deviation_300</th>\n",
       "      <th>log_ha_change</th>\n",
       "      <th>log_ha_change_intra</th>\n",
       "      <th>log_ha_change_intra_high</th>\n",
       "      <th>log_ha_change_intra_low</th>\n",
       "      <th>log_ha_high_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.012928</td>\n",
       "      <td>0.013987</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>-0.003281</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.009713</td>\n",
       "      <td>0.273865</td>\n",
       "      <td>0.070582</td>\n",
       "      <td>0.106799</td>\n",
       "      <td>-0.002194</td>\n",
       "      <td>-0.010152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.017736</td>\n",
       "      <td>0.017736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>-0.001525</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>-0.231275</td>\n",
       "      <td>0.071486</td>\n",
       "      <td>0.107933</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>0.008260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-0.010966</td>\n",
       "      <td>0.013219</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>-0.007461</td>\n",
       "      <td>-0.004173</td>\n",
       "      <td>-0.006793</td>\n",
       "      <td>0.517538</td>\n",
       "      <td>0.059751</td>\n",
       "      <td>0.096423</td>\n",
       "      <td>-0.005754</td>\n",
       "      <td>-0.004997</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>-0.010349</td>\n",
       "      <td>0.013219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.014857</td>\n",
       "      <td>0.015574</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>-0.005196</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.296428</td>\n",
       "      <td>0.073756</td>\n",
       "      <td>0.110679</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.008857</td>\n",
       "      <td>-0.006718</td>\n",
       "      <td>0.015574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.007144</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.039677</td>\n",
       "      <td>0.079242</td>\n",
       "      <td>0.116441</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>0.014690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>-0.015447</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>-0.016932</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>-0.015324</td>\n",
       "      <td>0.208525</td>\n",
       "      <td>0.018386</td>\n",
       "      <td>-0.017319</td>\n",
       "      <td>-0.009913</td>\n",
       "      <td>-0.002042</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>-0.011098</td>\n",
       "      <td>0.017550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263</th>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>-0.004528</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>-0.173034</td>\n",
       "      <td>0.019831</td>\n",
       "      <td>-0.015107</td>\n",
       "      <td>-0.007116</td>\n",
       "      <td>-0.008137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012697</td>\n",
       "      <td>0.012697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>-0.018622</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>-0.023310</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>-0.020676</td>\n",
       "      <td>0.202728</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>-0.033089</td>\n",
       "      <td>-0.006484</td>\n",
       "      <td>-0.010561</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>-0.024035</td>\n",
       "      <td>0.027653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>-0.014535</td>\n",
       "      <td>0.022741</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>-0.017206</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.013078</td>\n",
       "      <td>0.651889</td>\n",
       "      <td>-0.013339</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.018441</td>\n",
       "      <td>-0.023735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034797</td>\n",
       "      <td>0.034797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>-0.001426</td>\n",
       "      <td>0.025266</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>-0.010681</td>\n",
       "      <td>0.009255</td>\n",
       "      <td>0.417816</td>\n",
       "      <td>-0.014721</td>\n",
       "      <td>-0.047672</td>\n",
       "      <td>-0.010457</td>\n",
       "      <td>-0.022395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.032596</td>\n",
       "      <td>0.032596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3968 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      log_normalized_change  log_price_range  log_price_range_high  \\\n",
       "299                0.012928         0.013987              0.010706   \n",
       "300                0.001720         0.008260              0.006735   \n",
       "301               -0.010966         0.013219              0.005758   \n",
       "302                0.014857         0.015574              0.010378   \n",
       "303                0.006366         0.007144              0.006947   \n",
       "...                     ...              ...                   ...   \n",
       "4262              -0.015447         0.017550              0.000618   \n",
       "4263               0.001631         0.007836              0.003308   \n",
       "4264              -0.018622         0.027653              0.004343   \n",
       "4265              -0.014535         0.022741              0.005535   \n",
       "4266              -0.001426         0.025266              0.022223   \n",
       "\n",
       "      log_price_range_low  log_premarket_changes  log_Smart_Money  \\\n",
       "299             -0.003281               0.003214         0.009713   \n",
       "300             -0.001525              -0.000729         0.002449   \n",
       "301             -0.007461              -0.004173        -0.006793   \n",
       "302             -0.005196               0.005664         0.009193   \n",
       "303             -0.000197               0.001119         0.005247   \n",
       "...                   ...                    ...              ...   \n",
       "4262            -0.016932              -0.000124        -0.015324   \n",
       "4263            -0.004528               0.000301         0.001329   \n",
       "4264            -0.023310               0.002053        -0.020676   \n",
       "4265            -0.017206              -0.001457        -0.013078   \n",
       "4266            -0.003043              -0.010681         0.009255   \n",
       "\n",
       "      log_volume_deviation  log_norm_avg_deviation_200  \\\n",
       "299               0.273865                    0.070582   \n",
       "300              -0.231275                    0.071486   \n",
       "301               0.517538                    0.059751   \n",
       "302               0.296428                    0.073756   \n",
       "303               0.039677                    0.079242   \n",
       "...                    ...                         ...   \n",
       "4262              0.208525                    0.018386   \n",
       "4263             -0.173034                    0.019831   \n",
       "4264              0.202728                    0.001120   \n",
       "4265              0.651889                   -0.013339   \n",
       "4266              0.417816                   -0.014721   \n",
       "\n",
       "      log_norm_avg_deviation_300  log_ha_change  log_ha_change_intra  \\\n",
       "299                     0.106799      -0.002194            -0.010152   \n",
       "300                     0.107933       0.006601             0.001513   \n",
       "301                     0.096423      -0.005754            -0.004997   \n",
       "302                     0.110679       0.004595             0.002093   \n",
       "303                     0.116441       0.009702             0.010748   \n",
       "...                          ...            ...                  ...   \n",
       "4262                   -0.017319      -0.009913            -0.002042   \n",
       "4263                   -0.015107      -0.007116            -0.008137   \n",
       "4264                   -0.033089      -0.006484            -0.010561   \n",
       "4265                   -0.046932      -0.018441            -0.023735   \n",
       "4266                   -0.047672      -0.010457            -0.022395   \n",
       "\n",
       "      log_ha_change_intra_high  log_ha_change_intra_low  log_ha_high_low  \n",
       "299                   0.000000                -0.017736         0.017736  \n",
       "300                   0.006329                -0.001932         0.008260  \n",
       "301                   0.002870                -0.010349         0.013219  \n",
       "302                   0.008857                -0.006718         0.015574  \n",
       "303                   0.014690                 0.000000         0.014690  \n",
       "...                        ...                      ...              ...  \n",
       "4262                  0.006452                -0.011098         0.017550  \n",
       "4263                  0.000000                -0.012697         0.012697  \n",
       "4264                  0.003618                -0.024035         0.027653  \n",
       "4265                  0.000000                -0.034797         0.034797  \n",
       "4266                  0.000000                -0.032596         0.032596  \n",
       "\n",
       "[3968 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84769f2f",
   "metadata": {},
   "source": [
    "# Pytorch setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f5fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout, num_heads):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to match the expected shape of (batch_size, sequence_length, input_size)\n",
    "        x = x.view(-1, 1, x.shape[-1])\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        output = output.permute(1, 0, 2)  # (seq_len, batch_size, hidden_size)\n",
    "        attention_output, _ = self.attention(output, output, output)  # apply self-attention\n",
    "        output = output + attention_output  # add residual connection\n",
    "        output = output.permute(1, 0, 2)  # (batch_size, seq_len, hidden_size)\n",
    "        output = self.relu(output[:, -1, :])\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return torch.squeeze(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc58cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 35000\n",
    "learning_rate = 0.0005\n",
    "num_layers=5\n",
    "input_size = len(x_train.columns)\n",
    "hidden_size = 90\n",
    "output_size = 1\n",
    "dropout=.5\n",
    "num_heads=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dd83d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_list_to_dataframe(arrays,length):\n",
    "    # Convert the list of arrays to a single 2D array\n",
    "    array_2d = np.concatenate(arrays, axis=1)\n",
    "    \n",
    "    # Reshape the 2D array into a 5-column array\n",
    "    array_5col = np.reshape(array_2d, (-1, length))\n",
    "    \n",
    "    # Create a DataFrame from the 5-column array\n",
    "    df = pd.DataFrame(array_5col)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2040bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "certainty_2sd_counts=pd.DataFrame()\n",
    "certainty_4sd_counts=pd.DataFrame()\n",
    "certainty_5sd_counts=pd.DataFrame()\n",
    "certainty_6sd_counts=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5510f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db282c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffwa/anaconda3/envs/DL_new/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.72650 0\n",
      "Epoch: 1000, loss: 0.22662 0\n",
      "Epoch: 2000, loss: 0.11121 0\n",
      "Epoch: 3000, loss: 0.00007 0\n",
      "Epoch: 4000, loss: 0.00003 0\n",
      "Epoch: 5000, loss: 0.00001 0\n",
      "Epoch: 6000, loss: 0.02484 0\n",
      "Epoch: 7000, loss: 0.00004 0\n",
      "Epoch: 8000, loss: 0.00014 0\n",
      "Epoch: 9000, loss: 0.00002 0\n",
      "Epoch: 10000, loss: 0.11895 0\n",
      "Epoch: 11000, loss: 0.00013 0\n",
      "Epoch: 12000, loss: 0.00004 0\n",
      "Epoch: 13000, loss: 0.00003 0\n",
      "Epoch: 14000, loss: 0.00001 0\n",
      "Epoch: 15000, loss: 0.00001 0\n",
      "Epoch: 16000, loss: 0.00001 0\n",
      "Epoch: 17000, loss: 0.00001 0\n",
      "Epoch: 18000, loss: 0.00000 0\n",
      "Epoch: 19000, loss: 0.00005 0\n",
      "Epoch: 20000, loss: 0.00003 0\n",
      "Epoch: 21000, loss: 0.00001 0\n",
      "Epoch: 22000, loss: 0.00002 0\n",
      "Epoch: 23000, loss: 0.00002 0\n",
      "Epoch: 24000, loss: 0.00001 0\n",
      "Epoch: 25000, loss: 0.02522 0\n",
      "Epoch: 26000, loss: 0.02521 0\n",
      "Epoch: 27000, loss: 0.00001 0\n",
      "Epoch: 28000, loss: 0.00001 0\n",
      "Epoch: 29000, loss: 0.00001 0\n",
      "Epoch: 30000, loss: 0.00002 0\n",
      "Epoch: 31000, loss: 0.00001 0\n",
      "Epoch: 32000, loss: 0.00004 0\n",
      "Epoch: 33000, loss: 0.00000 0\n",
      "Epoch: 34000, loss: 0.00001 0\n",
      "Epoch: 0, loss: 0.67021 1\n",
      "Epoch: 1000, loss: 0.26325 1\n",
      "Epoch: 2000, loss: 0.15142 1\n",
      "Epoch: 3000, loss: 0.00109 1\n",
      "Epoch: 4000, loss: 0.00002 1\n",
      "Epoch: 5000, loss: 0.00005 1\n",
      "Epoch: 6000, loss: 0.00010 1\n",
      "Epoch: 7000, loss: 0.00002 1\n",
      "Epoch: 8000, loss: 0.01122 1\n",
      "Epoch: 9000, loss: 0.00002 1\n",
      "Epoch: 10000, loss: 0.00002 1\n",
      "Epoch: 11000, loss: 0.00005 1\n",
      "Epoch: 12000, loss: 0.00003 1\n",
      "Epoch: 13000, loss: 0.00008 1\n",
      "Epoch: 14000, loss: 0.00013 1\n",
      "Epoch: 15000, loss: 0.00001 1\n",
      "Epoch: 16000, loss: 0.00000 1\n",
      "Epoch: 17000, loss: 0.00000 1\n",
      "Epoch: 18000, loss: 0.00001 1\n",
      "Epoch: 19000, loss: 0.00005 1\n",
      "Epoch: 20000, loss: 0.00001 1\n",
      "Epoch: 21000, loss: 0.00004 1\n",
      "Epoch: 22000, loss: 0.00002 1\n",
      "Epoch: 23000, loss: 0.00000 1\n",
      "Epoch: 24000, loss: 0.00001 1\n",
      "Epoch: 25000, loss: 0.00001 1\n",
      "Epoch: 26000, loss: 0.00000 1\n",
      "Epoch: 27000, loss: 0.00001 1\n",
      "Epoch: 28000, loss: 0.00004 1\n",
      "Epoch: 29000, loss: 0.00001 1\n",
      "Epoch: 30000, loss: 0.00001 1\n",
      "Epoch: 31000, loss: 0.00000 1\n",
      "Epoch: 32000, loss: 0.00000 1\n",
      "Epoch: 33000, loss: 0.00001 1\n",
      "Epoch: 34000, loss: 0.00000 1\n",
      "Epoch: 0, loss: 0.67192 2\n",
      "Epoch: 1000, loss: 0.28869 2\n",
      "Epoch: 2000, loss: 0.08035 2\n",
      "Epoch: 3000, loss: 0.00269 2\n",
      "Epoch: 4000, loss: 0.00076 2\n",
      "Epoch: 5000, loss: 0.00003 2\n",
      "Epoch: 6000, loss: 0.00433 2\n",
      "Epoch: 7000, loss: 0.00003 2\n",
      "Epoch: 8000, loss: 0.00004 2\n",
      "Epoch: 9000, loss: 0.00002 2\n",
      "Epoch: 10000, loss: 0.00004 2\n",
      "Epoch: 11000, loss: 0.00000 2\n",
      "Epoch: 12000, loss: 0.00002 2\n",
      "Epoch: 13000, loss: 0.00451 2\n",
      "Epoch: 14000, loss: 0.00170 2\n",
      "Epoch: 15000, loss: 0.00035 2\n",
      "Epoch: 16000, loss: 0.00031 2\n",
      "Epoch: 17000, loss: 0.00020 2\n",
      "Epoch: 18000, loss: 0.00003 2\n",
      "Epoch: 19000, loss: 0.00001 2\n",
      "Epoch: 20000, loss: 0.00003 2\n",
      "Epoch: 21000, loss: 0.00000 2\n",
      "Epoch: 22000, loss: 0.00000 2\n",
      "Epoch: 23000, loss: 0.00012 2\n",
      "Epoch: 24000, loss: 0.00001 2\n",
      "Epoch: 25000, loss: 0.00000 2\n",
      "Epoch: 26000, loss: 0.00001 2\n",
      "Epoch: 27000, loss: 0.00000 2\n",
      "Epoch: 28000, loss: 0.00001 2\n",
      "Epoch: 29000, loss: 0.00001 2\n",
      "Epoch: 30000, loss: 0.00001 2\n",
      "Epoch: 31000, loss: 0.00000 2\n",
      "Epoch: 32000, loss: 0.00168 2\n",
      "Epoch: 33000, loss: 0.00001 2\n",
      "Epoch: 34000, loss: 0.00000 2\n",
      "Epoch: 0, loss: 0.72412 3\n",
      "Epoch: 1000, loss: 0.25791 3\n",
      "Epoch: 2000, loss: 0.15675 3\n",
      "Epoch: 3000, loss: 0.00637 3\n",
      "Epoch: 4000, loss: 0.23519 3\n",
      "Epoch: 5000, loss: 0.00003 3\n",
      "Epoch: 6000, loss: 0.00007 3\n",
      "Epoch: 7000, loss: 0.08489 3\n",
      "Epoch: 8000, loss: 0.01023 3\n",
      "Epoch: 9000, loss: 0.03360 3\n",
      "Epoch: 10000, loss: 0.00402 3\n",
      "Epoch: 11000, loss: 0.00463 3\n",
      "Epoch: 12000, loss: 0.00438 3\n",
      "Epoch: 13000, loss: 0.02928 3\n",
      "Epoch: 14000, loss: 0.00144 3\n",
      "Epoch: 15000, loss: 0.00013 3\n",
      "Epoch: 16000, loss: 0.00007 3\n",
      "Epoch: 17000, loss: 0.00002 3\n",
      "Epoch: 18000, loss: 0.00004 3\n",
      "Epoch: 19000, loss: 0.00014 3\n",
      "Epoch: 20000, loss: 0.00005 3\n",
      "Epoch: 21000, loss: 0.00008 3\n",
      "Epoch: 22000, loss: 0.00001 3\n",
      "Epoch: 23000, loss: 0.00002 3\n",
      "Epoch: 24000, loss: 0.00001 3\n",
      "Epoch: 25000, loss: 0.00002 3\n",
      "Epoch: 26000, loss: 0.00012 3\n",
      "Epoch: 27000, loss: 0.01777 3\n",
      "Epoch: 28000, loss: 0.00001 3\n",
      "Epoch: 29000, loss: 0.00014 3\n",
      "Epoch: 30000, loss: 0.00001 3\n",
      "Epoch: 31000, loss: 0.01315 3\n",
      "Epoch: 32000, loss: 0.00001 3\n",
      "Epoch: 33000, loss: 0.00003 3\n",
      "Epoch: 34000, loss: 0.00001 3\n",
      "Epoch: 0, loss: 0.70997 4\n",
      "Epoch: 1000, loss: 0.26592 4\n",
      "Epoch: 2000, loss: 0.11006 4\n",
      "Epoch: 3000, loss: 0.07892 4\n",
      "Epoch: 4000, loss: 0.00508 4\n",
      "Epoch: 5000, loss: 0.00212 4\n",
      "Epoch: 6000, loss: 0.00002 4\n",
      "Epoch: 7000, loss: 0.00002 4\n",
      "Epoch: 8000, loss: 0.00001 4\n",
      "Epoch: 9000, loss: 0.30537 4\n",
      "Epoch: 10000, loss: 0.00456 4\n",
      "Epoch: 11000, loss: 0.00160 4\n",
      "Epoch: 12000, loss: 0.00085 4\n",
      "Epoch: 13000, loss: 0.00045 4\n",
      "Epoch: 14000, loss: 0.00026 4\n",
      "Epoch: 15000, loss: 0.00003 4\n",
      "Epoch: 16000, loss: 0.00076 4\n",
      "Epoch: 17000, loss: 0.00001 4\n",
      "Epoch: 18000, loss: 0.00002 4\n",
      "Epoch: 19000, loss: 0.00000 4\n",
      "Epoch: 20000, loss: 0.00001 4\n",
      "Epoch: 21000, loss: 0.00001 4\n",
      "Epoch: 22000, loss: 0.00001 4\n",
      "Epoch: 23000, loss: 0.00001 4\n",
      "Epoch: 24000, loss: 0.00002 4\n",
      "Epoch: 25000, loss: 0.00001 4\n",
      "Epoch: 26000, loss: 0.00001 4\n",
      "Epoch: 27000, loss: 0.00001 4\n",
      "Epoch: 28000, loss: 0.00003 4\n",
      "Epoch: 29000, loss: 0.00001 4\n",
      "Epoch: 30000, loss: 0.00002 4\n",
      "Epoch: 31000, loss: 0.00001 4\n",
      "Epoch: 32000, loss: 0.00000 4\n",
      "Epoch: 33000, loss: 0.00005 4\n",
      "Epoch: 34000, loss: 0.00000 4\n",
      "Epoch: 0, loss: 0.68317 5\n",
      "Epoch: 1000, loss: 0.26574 5\n",
      "Epoch: 2000, loss: 0.12794 5\n",
      "Epoch: 3000, loss: 0.58697 5\n",
      "Epoch: 4000, loss: 0.00752 5\n",
      "Epoch: 5000, loss: 0.00367 5\n",
      "Epoch: 6000, loss: 0.00724 5\n",
      "Epoch: 7000, loss: 0.14218 5\n",
      "Epoch: 8000, loss: 0.00114 5\n",
      "Epoch: 9000, loss: 0.00091 5\n",
      "Epoch: 10000, loss: 0.00051 5\n",
      "Epoch: 11000, loss: 0.00019 5\n",
      "Epoch: 12000, loss: 0.00026 5\n",
      "Epoch: 13000, loss: 0.00064 5\n",
      "Epoch: 14000, loss: 0.00020 5\n",
      "Epoch: 15000, loss: 0.00026 5\n",
      "Epoch: 16000, loss: 0.00009 5\n",
      "Epoch: 17000, loss: 0.00014 5\n",
      "Epoch: 18000, loss: 0.00006 5\n",
      "Epoch: 19000, loss: 0.00003 5\n",
      "Epoch: 20000, loss: 0.00010 5\n",
      "Epoch: 21000, loss: 0.00002 5\n",
      "Epoch: 22000, loss: 0.00001 5\n",
      "Epoch: 23000, loss: 0.00049 5\n",
      "Epoch: 24000, loss: 0.00018 5\n",
      "Epoch: 25000, loss: 0.00016 5\n",
      "Epoch: 26000, loss: 0.00002 5\n",
      "Epoch: 27000, loss: 0.00015 5\n",
      "Epoch: 28000, loss: 0.00005 5\n",
      "Epoch: 29000, loss: 0.00006 5\n",
      "Epoch: 30000, loss: 0.00003 5\n",
      "Epoch: 31000, loss: 0.00002 5\n",
      "Epoch: 32000, loss: 0.00002 5\n",
      "Epoch: 33000, loss: 0.00005 5\n",
      "Epoch: 34000, loss: 0.00005 5\n",
      "Epoch: 0, loss: 0.67503 6\n",
      "Epoch: 1000, loss: 0.22747 6\n",
      "Epoch: 2000, loss: 0.13341 6\n",
      "Epoch: 3000, loss: 0.11639 6\n",
      "Epoch: 4000, loss: 0.07191 6\n",
      "Epoch: 5000, loss: 0.07551 6\n",
      "Epoch: 6000, loss: 0.07258 6\n",
      "Epoch: 7000, loss: 0.05719 6\n",
      "Epoch: 8000, loss: 0.04450 6\n",
      "Epoch: 9000, loss: 0.02328 6\n",
      "Epoch: 10000, loss: 0.00176 6\n",
      "Epoch: 11000, loss: 0.00007 6\n",
      "Epoch: 12000, loss: 0.00000 6\n",
      "Epoch: 13000, loss: 0.00003 6\n",
      "Epoch: 14000, loss: 0.00001 6\n",
      "Epoch: 15000, loss: 0.00008 6\n",
      "Epoch: 16000, loss: 0.00001 6\n",
      "Epoch: 17000, loss: 0.01880 6\n",
      "Epoch: 18000, loss: 0.00001 6\n",
      "Epoch: 19000, loss: 0.00001 6\n",
      "Epoch: 20000, loss: 0.00001 6\n",
      "Epoch: 21000, loss: 0.58969 6\n",
      "Epoch: 22000, loss: 0.00004 6\n",
      "Epoch: 23000, loss: 0.00003 6\n",
      "Epoch: 24000, loss: 0.00001 6\n",
      "Epoch: 25000, loss: 0.00001 6\n",
      "Epoch: 26000, loss: 0.00000 6\n",
      "Epoch: 27000, loss: 0.00001 6\n",
      "Epoch: 28000, loss: 0.00024 6\n",
      "Epoch: 29000, loss: 0.00002 6\n",
      "Epoch: 30000, loss: 0.00001 6\n",
      "Epoch: 31000, loss: 0.00003 6\n",
      "Epoch: 32000, loss: 0.00001 6\n",
      "Epoch: 33000, loss: 0.00001 6\n",
      "Epoch: 34000, loss: 0.00001 6\n"
     ]
    }
   ],
   "source": [
    "for j in range(7):\n",
    "    # Instantiate the model\n",
    "    model = LSTM(input_size=input_size, hidden_size=hidden_size,num_layers=num_layers,output_size=1,dropout=dropout,num_heads=num_heads)#output_size=1\n",
    "    model=torch.compile(model)\n",
    "    model.to(device)\n",
    "    train_data_x=train_data.X.to(device)\n",
    "    train_data_y=train_data.Y.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.BCELoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.003,weight_decay=6e-7) \n",
    "        # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        outputs = model(train_data_x)\n",
    "        loss = criterion(outputs, train_data_y.squeeze())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Epoch: %d, loss: %1.5f\"  % (epoch, loss.item()),j)\n",
    "        \"\"\"for batch_X, batch_Y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_Y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\"\"\"\n",
    "        \n",
    "        \n",
    "    #Transform test data\n",
    "    transformedData = scaler.transform(x_test)\n",
    "    transformedData_y=test_y.values.reshape(-1, 1)\n",
    "\n",
    "    # Use the trained model for predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_data = MyDataset(transformedData, transformedData_y)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "    test_data_x=test_data.X.to(device)\n",
    "    \n",
    "    outputlist=[]\n",
    "    \n",
    "    #testing data\n",
    "    \n",
    "    model.train()\n",
    "    len_outputs=100\n",
    "    with torch.no_grad():\n",
    "        for i in range(len_outputs):\n",
    "            output = model(test_data_x)\n",
    "            outputlist.append(output)\n",
    "    model.eval()\n",
    "    \n",
    "    scaled_output_list=[]\n",
    "    for outputs in outputlist:\n",
    "        scaled_output=outputs.cpu().reshape(-1, 1)\n",
    "        scaled_output_list.append(scaled_output)\n",
    "        \n",
    "    model_test_outputs=array_list_to_dataframe(scaled_output_list,len_outputs )\n",
    "    \n",
    "    count1 = (model_test_outputs > .999).sum(axis=1)\n",
    "    count2=(model_test_outputs > .9999).sum(axis=1)\n",
    "    count3=(model_test_outputs > .99999).sum(axis=1)\n",
    "    count4=(model_test_outputs > .999999).sum(axis=1)\n",
    "    \n",
    "    \n",
    "    column_name = 'Column ' + str(j)\n",
    "    \n",
    "    certainty_2sd_counts[column_name] =count1 \n",
    "    certainty_4sd_counts[column_name] =count2\n",
    "    certainty_5sd_counts[column_name] =count3\n",
    "    certainty_6sd_counts[column_name] =count4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6782bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     10.571429\n",
       "1     44.285714\n",
       "2     29.000000\n",
       "3     41.714286\n",
       "4      0.000000\n",
       "5     12.714286\n",
       "6      0.142857\n",
       "7      0.000000\n",
       "8     42.571429\n",
       "9     23.428571\n",
       "10    45.428571\n",
       "11    68.285714\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certainty_4sd_counts.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b58fdfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column 0</th>\n",
       "      <th>Column 1</th>\n",
       "      <th>Column 2</th>\n",
       "      <th>Column 3</th>\n",
       "      <th>Column 4</th>\n",
       "      <th>Column 5</th>\n",
       "      <th>Column 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>98</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column 0  Column 1  Column 2  Column 3  Column 4  Column 5  Column 6\n",
       "0         74         0         0         0         0         0         0\n",
       "1          0        25       100        98         0        84         3\n",
       "2         80         0         0        39         0        84         0\n",
       "3          0         5         0        98         0        89       100\n",
       "4          0         0         0         0         0         0         0\n",
       "5         89         0         0         0         0         0         0\n",
       "6          1         0         0         0         0         0         0\n",
       "7          0         0         0         0         0         0         0\n",
       "8          0        96        97         0         5         0       100\n",
       "9          0        98         0         0         0        66         0\n",
       "10         0       100        99         0         0        19       100\n",
       "11       100        91         0        94        98        95         0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certainty_4sd_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d40e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
